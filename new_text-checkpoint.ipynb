{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4cdd5-37a7-4b94-a9c9-0f0199413d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b5051-c631-47e6-ba39-1e8235225000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt -t /path/to/directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0ea6b-b1d1-4532-92eb-6a54e1a64a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install corenlp -t /path/to/directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7f7496-1006-4bc6-8b14-1a7b47550b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import stanza\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser\n",
    "from stanza.server import CoreNLPClient\n",
    "from stanza.server.ud_enhancer import UniversalEnhancer\n",
    "\n",
    "import dictionary_funcs\n",
    "import project_config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9debf514-c0aa-4736-adf9-144abc54edcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigram_model = Phraser.load(str(cfg.DIR.models / \"phrases\" / \"bi_phrase.mod\"))\n",
    "trigram_model = Phraser.load(str(cfg.DIR.models / \"phrases\" / \"tri_phrase.mod\"))\n",
    "w2v_model = Word2Vec.load(str(cfg.DIR.models / \"w2v\" / \"w2v.mod\"))\n",
    "# read document-freq\n",
    "with open(str(cfg.DIR.models / \"df_dict.pkl\"), \"rb\") as f:\n",
    "    df_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c309d0-1aab-4af9-805a-77d8213c2d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 06:20:59 INFO: Writing properties to tmp file: corenlp_server-e6be6410c5144a25.props\n",
      "2023-12-21 06:20:59 INFO: Connecting to existing CoreNLP server at localhost:8888\n"
     ]
    }
   ],
   "source": [
    "client = CoreNLPClient(properties={\"ner.applyFineGrained\": \"false\", \"annotators\": \"tokenize, ssplit, pos, lemma, ner, depparse\",}, endpoint=\"http://localhost:8888\", start_server=stanza.server.StartServer.TRY_START,\n",
    "            timeout=120000000,be_quiet=True,)\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635d06c-a7cd-4fe6-9e8b-9740a18aee0a",
   "metadata": {},
   "source": [
    "# Clean and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d763da0f-3c28-4473-9c7d-70168000307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_NER(line):\n",
    "    NERs = re.compile(\"(\\[NER:\\w+\\])(\\S+)\")\n",
    "    line = re.sub(NERs, r\"\\1\", line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def remove_puct_num(line):\n",
    "    tokens = line.strip().lower().split()\n",
    "    tokens = [re.sub(\"\\[pos:.*?\\]\", \"\", t) for t in tokens]\n",
    "    # these are tagged bracket and parenthesises\n",
    "    if cfg.options.REMOVE_STOPWORDS:\n",
    "        puncts_stops = (\n",
    "            set([\"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"'s\"]) | cfg.options.STOPWORDS\n",
    "        )\n",
    "    else:\n",
    "        puncts_stops = set([\"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"'s\"])\n",
    "    # filter out numerics and 1-letter words as recommend by https://sraf.nd.edu/textual-analysis/resources/#StopWords\n",
    "    tokens = filter(\n",
    "        lambda t: any(c.isalpha() for c in t) and t not in puncts_stops and len(t) > 1,\n",
    "        tokens,\n",
    "    )\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    \"\"\"Main function that chains all filters together and applies to a string.\"\"\"\n",
    "    lines = doc.split(\"\\n\")\n",
    "    cleaned = [functools.reduce(lambda obj, func: func(obj), [remove_NER, remove_puct_num], line,) for line in lines]\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def sentence_mwe_finder(\n",
    "    sentence_ann, dep_types=set([\"mwe\", \"compound\", \"compound:prt\", \"fixed\"])):\n",
    "    WMEs = [x for x in sentence_ann.enhancedPlusPlusDependencies.edge if x.dep in dep_types]\n",
    "    wme_edges = []\n",
    "    for wme in WMEs:\n",
    "        edge = sorted([wme.target, wme.source])\n",
    "        # Note: (-1) because edges in WMEs use indicies that indicate the end of a token (tokenEndIndex)\n",
    "        # (+ sentence_ann.token[0].tokenBeginIndex) because\n",
    "        # the edges indices are for current sentence, whereas tokenBeginIndex are for the document.\n",
    "        wme_edges.append([end - 1 + sentence_ann.token[0].tokenBeginIndex for end in edge])\n",
    "    return wme_edges\n",
    "\n",
    "def sentence_NE_finder(sentence_ann):\n",
    "    NE_edges = []\n",
    "    NE_types = []\n",
    "    for m in sentence_ann.mentions:\n",
    "        edge = sorted([m.tokenStartInSentenceInclusive, m.tokenEndInSentenceExclusive])\n",
    "        # Note: edge in NEs's end index is at the end of the last token\n",
    "        NE_edges.append([edge[0], edge[1] - 1])\n",
    "        NE_types.append(m.entityType)\n",
    "    return NE_edges, NE_types\n",
    "\n",
    "\n",
    "def edge_simplifier(edges):\n",
    "    edge_sources = set([])  # edge that connects next token\n",
    "    for e in edges:\n",
    "        if e[0] + 1 == e[1]:\n",
    "            edge_sources.add(e[0])\n",
    "        else:\n",
    "            for i in range(e[0], e[1]):\n",
    "                edge_sources.add(i)\n",
    "    return edge_sources\n",
    "\n",
    "def process_document(doc):\n",
    "    with CoreNLPClient(endpoint=\"http://localhost:8888\", start_server=stanza.server.StartServer.DONT_START,timeout=120000000,be_quiet=True,) as client:\n",
    "        doc_ann = client.annotate(doc)\n",
    "    sentences_processed = []\n",
    "    for i, sentence in enumerate(doc_ann.sentence):\n",
    "        sentences_processed.append(process_sentence(sentence))\n",
    "    return \"\\n\".join(sentences_processed)\n",
    "\n",
    "def process_sentence(sentence_ann):\n",
    "    mwe_edge_sources = edge_simplifier(sentence_mwe_finder(sentence_ann))\n",
    "    # NE_edges can span more than two words or self-pointing\n",
    "    NE_edges, NE_types = sentence_NE_finder(sentence_ann)\n",
    "    # For tagging NEs\n",
    "    NE_BeginIndices = [e[0] for e in NE_edges]\n",
    "    # Unpack NE_edges to two-word edges set([i,j],..)\n",
    "    NE_edge_sources = edge_simplifier(NE_edges)\n",
    "    # For concat MWEs, multi-words NEs are MWEs too\n",
    "    mwe_edge_sources |= NE_edge_sources\n",
    "    sentence_parsed = []\n",
    "\n",
    "    NE_j = 0\n",
    "    for i, t in enumerate(sentence_ann.token):\n",
    "        token_lemma = \"{}[pos:{}]\".format(t.lemma, t.pos)\n",
    "        # concate MWEs\n",
    "        if t.tokenBeginIndex not in mwe_edge_sources:\n",
    "            token_lemma = token_lemma + \" \"\n",
    "        else:\n",
    "            token_lemma = token_lemma + \"_\"\n",
    "        # Add NE tags\n",
    "        if t.tokenBeginIndex in NE_BeginIndices:\n",
    "            if t.ner != \"O\":\n",
    "                # Only add tag if the word itself is an entity.\n",
    "                # (If a Pronoun refers to an entity, mention will also tag it.)\n",
    "                token_lemma = \"[NER:{}]\".format(NE_types[NE_j]) + token_lemma\n",
    "                NE_j += 1\n",
    "        sentence_parsed.append(token_lemma)\n",
    "    return \"\".join(sentence_parsed)\n",
    "\n",
    "# Concatenate must-have phrases in the text.\n",
    "def concat_must_have_phrases(text):\n",
    "    all_seeds = []  # Load your must-have phrases here\n",
    "    pattern = \"|\".join(\n",
    "        map(re.escape, [phrase.replace(\"_\", \" \") for phrase in all_seeds])\n",
    "    )\n",
    "    text = re.sub(pattern, lambda match: match.group().replace(\" \", \"_\"), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d912a08-51a0-4736-a129-e55469a0b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process new text by applying all models and return its vectorized form.\n",
    "def clean_and_vectorize(doc_processed):\n",
    "    print(doc_processed)\n",
    "    doc_cleaned = clean(doc_processed)\n",
    "    print(doc_cleaned)\n",
    "    concatenated_text = concat_must_have_phrases(doc_cleaned)\n",
    "    print(concatenated_text)\n",
    "    phrase_applied_text = apply_phrase_models(concatenated_text)\n",
    "    print(phrase_applied_text)\n",
    "    vectorized_text = vectorize_text(phrase_applied_text)\n",
    "    vectorized_text = np.mean(vectorized_text, axis=0)\n",
    "    vectorized_text = vectorized_text / np.linalg.norm(vectorized_text)\n",
    "\n",
    "    # ver2.0: weighted average using tf-idf\n",
    "    # compute the tf-idf weighted average of all vectors\n",
    "    doc_weighted = []\n",
    "    for word in phrase_applied_text.split():\n",
    "        if word in w2v_model.wv:\n",
    "            word_vector = w2v_model.wv[word]\n",
    "            word_weight = np.log(1 + df_dict[word])\n",
    "            doc_weighted.append(word_vector * word_weight)\n",
    "    vectorized_text_weighted = np.mean(doc_weighted, axis=0)\n",
    "    # normalize the vector length\n",
    "    vectorized_text_weighted = vectorized_text_weighted / np.linalg.norm(\n",
    "        vectorized_text_weighted)\n",
    "    return vectorized_text, vectorized_text_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aee5d9-8a2e-4011-a736-92ca6264d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 06:22:38 INFO: Connecting to existing CoreNLP server at localhost:8888\n"
     ]
    }
   ],
   "source": [
    "client.annotate(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b946a1a-dec6-476b-ad68-3ba2296e4fb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnnotationException",
     "evalue": "<html><title>403: Forbidden</title><body>403: Forbidden</body></html>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\Ivey\\Python\\Lib\\site-packages\\stanza\\server\\client.py:465\u001b[0m, in \u001b[0;36mCoreNLPClient._request\u001b[1;34m(self, buf, properties, reset_default, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[0;32m    462\u001b[0m                   params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(properties), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresetDefault\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(reset_default)\u001b[38;5;241m.\u001b[39mlower()},\n\u001b[0;32m    463\u001b[0m                   data\u001b[38;5;241m=\u001b[39mbuf, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m: ctype},\n\u001b[0;32m    464\u001b[0m                   timeout\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 465\u001b[0m r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\Documents\\Ivey\\Python\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: http://localhost:8888/?properties=%7B%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAnnotationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey to this ecosystem is the network approach, which leverages cloud networks and information networks to support a wide range of services. This approach enables businesses to offer services and support solutions more effectively. By utilizing application expertise and a diverse skill set, companies can co-engineer and join together multiple modalities to create comprehensive business process solutions. These solutions often include video content management and BPM (Business Process Management) solutions, which are essential in today\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms digital landscape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m doc_processed \u001b[38;5;241m=\u001b[39m process_document(doc)\n\u001b[0;32m      4\u001b[0m vectorized_new_text \u001b[38;5;241m=\u001b[39m clean_and_vectorize(doc_processed)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorized_new_text)\n",
      "Cell \u001b[1;32mIn[12], line 67\u001b[0m, in \u001b[0;36mprocess_document\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_document\u001b[39m(doc):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CoreNLPClient(endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8888\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_server\u001b[38;5;241m=\u001b[39mstanza\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mStartServer\u001b[38;5;241m.\u001b[39mDONT_START,timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120000000\u001b[39m,be_quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m---> 67\u001b[0m         doc_ann \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mannotate(doc)\n\u001b[0;32m     68\u001b[0m     sentences_processed \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(doc_ann\u001b[38;5;241m.\u001b[39msentence):\n",
      "File \u001b[1;32m~\\Documents\\Ivey\\Python\\Lib\\site-packages\\stanza\\server\\client.py:536\u001b[0m, in \u001b[0;36mCoreNLPClient.annotate\u001b[1;34m(self, text, annotators, output_format, properties, reset_default, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     reset_default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 536\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m), request_properties, reset_default, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputFormat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\Documents\\Ivey\\Python\\Lib\\site-packages\\stanza\\server\\client.py:471\u001b[0m, in \u001b[0;36mCoreNLPClient._request\u001b[1;34m(self, buf, properties, reset_default, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AnnotationException(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs:\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AnnotationException(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mAnnotationException\u001b[0m: <html><title>403: Forbidden</title><body>403: Forbidden</body></html>"
     ]
    }
   ],
   "source": [
    "doc = \"Key to this ecosystem is the network approach, which leverages cloud networks and information networks to support a wide range of services. This approach enables businesses to offer services and support solutions more effectively. By utilizing application expertise and a diverse skill set, companies can co-engineer and join together multiple modalities to create comprehensive business process solutions. These solutions often include video content management and BPM (Business Process Management) solutions, which are essential in today's digital landscape.\"\n",
    "\n",
    "doc_processed = process_document(doc)\n",
    "vectorized_new_text = clean_and_vectorize(doc_processed)[1]\n",
    "\n",
    "print(vectorized_new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43620189-221e-46ae-8b8d-4ccc21812e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(doc)\n",
    "\n",
    "\n",
    "    # project text to expanded dict of each aspect\n",
    "    marketing_aspects = [\n",
    "        \"marketing_capabilities\",\n",
    "        \"marketing_excellence\",\n",
    "        \"marketing_orientation\",\n",
    "    ]\n",
    "    aspect = marketing_aspects[0]  # using marketing_capabilities as an example\n",
    "\n",
    "    out_put_dir = \"outputs\"\n",
    "    dict_path = str(\n",
    "        Path(out_put_dir, aspect, \"n_words=2000\", \"expanded_dict_final.csv\")\n",
    "    )\n",
    "    expanded_dict = dictionary_funcs.read_dict_from_csv(dict_path)[0]\n",
    "\n",
    "    # vectorize each dim of the expanded dict, as the average of all words in the dim\n",
    "    expanded_dict_vectors = {}\n",
    "    for dim in expanded_dict:\n",
    "        dim_vector = []\n",
    "        for word in expanded_dict[dim]:\n",
    "            if word in w2v_model.wv:\n",
    "                dim_vector.append(w2v_model.wv[word])\n",
    "        avg_dim_vector = np.mean(dim_vector, axis=0)\n",
    "        # normalize the vector length\n",
    "        avg_dim_vector = avg_dim_vector / np.linalg.norm(avg_dim_vector)\n",
    "        expanded_dict_vectors[dim] = avg_dim_vector\n",
    "    print(f\"expanded_dict_vectors's keys: {expanded_dict_vectors.keys()}\")\n",
    "\n",
    "    \n",
    "    # calculate the cosine similarity between doc_vector and each dim_vector in the expanded_dict_vectors\n",
    "    aspect_scores = {}\n",
    "    for dim in expanded_dict_vectors:\n",
    "        aspect_scores[dim] = np.dot(vectorized_new_text, expanded_dict_vectors[dim])\n",
    "\n",
    "    print(aspect_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd1d98-ee76-4fd5-94d6-290758d23564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
