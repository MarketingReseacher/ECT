{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a543297-cee6-47d7-9239-4cd4b551dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 14:27:55 INFO: Writing properties to tmp file: corenlp_server-3f0beb14286f4a5f.props\n",
      "2023-12-21 14:27:55 INFO: Connecting to existing CoreNLP server at localhost:9002\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter text Key to this ecosystem is the network approach, which leverages cloud networks and information networks to support a wide range of services\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key[pos:JJ] to[pos:IN] this[pos:DT] ecosystem[pos:NN] be[pos:VBZ] the[pos:DT] network[pos:NN]_approach[pos:NN] ,[pos:,] which[pos:WDT] leverage[pos:VBZ] cloud[pos:NN]_network[pos:NNS] and[pos:CC] information[pos:NN]_network[pos:NNS] to[pos:TO] support[pos:VB] a[pos:DT] wide[pos:JJ] range[pos:NN] of[pos:IN] service[pos:NNS] \n",
      "key ecosystem network_approach leverage cloud_network information_network support wide range service\n",
      "key ecosystem network_approach leverage cloud_network information_network support wide range service\n",
      "key ecosystem network_approach leverage cloud_network information_network support wide_range service\n",
      "Importing dict: outputs\\marketing_capabilities\\expanded_dict.csv\n",
      "Number of words in marketing_ecosystem dimension: 1313\n",
      "Number of words in end_user dimension: 1570\n",
      "Number of words in marketing_agility dimension: 1004\n",
      "{'marketing_ecosystem': 0.7737333, 'end_user': 0.6269506, 'marketing_agility': 0.5761062}\n",
      "Importing dict: outputs\\marketing_excellence\\expanded_dict.csv\n",
      "Number of words in marketing_information_management dimension: 1912\n",
      "Number of words in marketing_planning_capabilities dimension: 1165\n",
      "Number of words in marketing_implementation_capabilities dimension: 693\n",
      "Number of words in pricing_capabilities dimension: 1652\n",
      "Number of words in product_development_capabilities dimension: 1505\n",
      "Number of words in channel_management dimension: 1228\n",
      "Number of words in marketing_communication_capabilities dimension: 1252\n",
      "Number of words in selling_capabilities dimension: 1383\n",
      "{'marketing_information_management': 0.575997, 'marketing_planning_capabilities': 0.47498018, 'marketing_implementation_capabilities': 0.5477318, 'pricing_capabilities': 0.030806903, 'product_development_capabilities': 0.579505, 'channel_management': 0.55651975, 'marketing_communication_capabilities': 0.3557419, 'selling_capabilities': 0.41683978}\n",
      "Importing dict: outputs\\marketing_orientation\\expanded_dict.csv\n",
      "Number of words in customer_orientation dimension: 1759\n",
      "Number of words in competitor_orientation dimension: 1978\n",
      "Number of words in interfunctional_coordination dimension: 1444\n",
      "Number of words in long-term_focus dimension: 1708\n",
      "Number of words in profit_focus dimension: 1923\n",
      "Number of words in intelligence_generation dimension: 1276\n",
      "Number of words in intelligence_dissemination dimension: 1467\n",
      "Number of words in responsiveness dimension: 781\n",
      "{'customer_orientation': 0.36487642, 'competitor_orientation': -0.04106971, 'interfunctional_coordination': 0.6489688, 'long-term_focus': 0.25634265, 'profit_focus': -0.052389756, 'intelligence_generation': 0.5265529, 'intelligence_dissemination': 0.6395452, 'responsiveness': 0.5473139}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import stanza\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser\n",
    "from stanza.server import CoreNLPClient\n",
    "from stanza.server.ud_enhancer import UniversalEnhancer\n",
    "\n",
    "import dictionary_funcs\n",
    "import project_config as cfg\n",
    "\n",
    "bigram_model = Phraser.load(str(cfg.DIR.models / \"phrases\" / \"bi_phrase.mod\"))\n",
    "trigram_model = Phraser.load(str(cfg.DIR.models / \"phrases\" / \"tri_phrase.mod\"))\n",
    "w2v_model = Word2Vec.load(str(cfg.DIR.models / \"w2v\" / \"w2v.mod\"))\n",
    "# read document-freq\n",
    "with open(str(cfg.DIR.models / \"df_dict.pkl\"), \"rb\") as f:\n",
    "    df_dict = pickle.load(f)\n",
    "\n",
    "client = CoreNLPClient(properties={\"ner.applyFineGrained\": \"false\", \"annotators\": \"tokenize, ssplit, pos, lemma, ner, depparse\",}, endpoint=\"http://localhost:9002\", start_server=stanza.server.StartServer.TRY_START,\n",
    "            timeout=120000000,be_quiet=True,)\n",
    "client.start()\n",
    "\n",
    "\n",
    "def remove_NER(line):\n",
    "    NERs = re.compile(\"(\\[NER:\\w+\\])(\\S+)\")\n",
    "    line = re.sub(NERs, r\"\\1\", line)\n",
    "    return line\n",
    "\n",
    "\n",
    "def remove_puct_num(line):\n",
    "    tokens = line.strip().lower().split()\n",
    "    tokens = [re.sub(\"\\[pos:.*?\\]\", \"\", t) for t in tokens]\n",
    "    # these are tagged bracket and parenthesises\n",
    "    if cfg.options.REMOVE_STOPWORDS:\n",
    "        puncts_stops = (\n",
    "            set([\"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"'s\"]) | cfg.options.STOPWORDS\n",
    "        )\n",
    "    else:\n",
    "        puncts_stops = set([\"-lrb-\", \"-rrb-\", \"-lsb-\", \"-rsb-\", \"'s\"])\n",
    "    # filter out numerics and 1-letter words as recommend by https://sraf.nd.edu/textual-analysis/resources/#StopWords\n",
    "    tokens = filter(\n",
    "        lambda t: any(c.isalpha() for c in t) and t not in puncts_stops and len(t) > 1,\n",
    "        tokens,\n",
    "    )\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Main function that chains all filters together and applies to a string.\n",
    "def clean(doc):\n",
    "    lines = doc.split(\"\\n\")\n",
    "    cleaned = [functools.reduce(lambda obj, func: func(obj), [remove_NER, remove_puct_num], line,) for line in lines]\n",
    "    return \"\\n\".join(cleaned)\n",
    "\n",
    "\n",
    "def sentence_mwe_finder(\n",
    "    sentence_ann, dep_types=set([\"mwe\", \"compound\", \"compound:prt\", \"fixed\"])):\n",
    "    WMEs = [x for x in sentence_ann.enhancedPlusPlusDependencies.edge if x.dep in dep_types]\n",
    "    wme_edges = []\n",
    "    for wme in WMEs:\n",
    "        edge = sorted([wme.target, wme.source])\n",
    "        # Note: (-1) because edges in WMEs use indicies that indicate the end of a token (tokenEndIndex)\n",
    "        # (+ sentence_ann.token[0].tokenBeginIndex) because\n",
    "        # the edges indices are for current sentence, whereas tokenBeginIndex are for the document.\n",
    "        wme_edges.append([end - 1 + sentence_ann.token[0].tokenBeginIndex for end in edge])\n",
    "    return wme_edges\n",
    "\n",
    "def sentence_NE_finder(sentence_ann):\n",
    "    NE_edges = []\n",
    "    NE_types = []\n",
    "    for m in sentence_ann.mentions:\n",
    "        edge = sorted([m.tokenStartInSentenceInclusive, m.tokenEndInSentenceExclusive])\n",
    "        # Note: edge in NEs's end index is at the end of the last token\n",
    "        NE_edges.append([edge[0], edge[1] - 1])\n",
    "        NE_types.append(m.entityType)\n",
    "    return NE_edges, NE_types\n",
    "\n",
    "\n",
    "def edge_simplifier(edges):\n",
    "    edge_sources = set([])  # edge that connects next token\n",
    "    for e in edges:\n",
    "        if e[0] + 1 == e[1]:\n",
    "            edge_sources.add(e[0])\n",
    "        else:\n",
    "            for i in range(e[0], e[1]):\n",
    "                edge_sources.add(i)\n",
    "    return edge_sources\n",
    "\n",
    "def process_document(doc):\n",
    "    with CoreNLPClient(endpoint=\"http://localhost:9002\", start_server=stanza.server.StartServer.DONT_START,timeout=120000000,be_quiet=True,) as client:\n",
    "        doc_ann = client.annotate(doc)\n",
    "    sentences_processed = []\n",
    "    for i, sentence in enumerate(doc_ann.sentence):\n",
    "        sentences_processed.append(process_sentence(sentence))\n",
    "    return \"\\n\".join(sentences_processed)\n",
    "\n",
    "def process_sentence(sentence_ann):\n",
    "    mwe_edge_sources = edge_simplifier(sentence_mwe_finder(sentence_ann))\n",
    "    # NE_edges can span more than two words or self-pointing\n",
    "    NE_edges, NE_types = sentence_NE_finder(sentence_ann)\n",
    "    # For tagging NEs\n",
    "    NE_BeginIndices = [e[0] for e in NE_edges]\n",
    "    # Unpack NE_edges to two-word edges set([i,j],..)\n",
    "    NE_edge_sources = edge_simplifier(NE_edges)\n",
    "    # For concat MWEs, multi-words NEs are MWEs too\n",
    "    mwe_edge_sources |= NE_edge_sources\n",
    "    sentence_parsed = []\n",
    "\n",
    "    NE_j = 0\n",
    "    for i, t in enumerate(sentence_ann.token):\n",
    "        token_lemma = \"{}[pos:{}]\".format(t.lemma, t.pos)\n",
    "        # concate MWEs\n",
    "        if t.tokenBeginIndex not in mwe_edge_sources:\n",
    "            token_lemma = token_lemma + \" \"\n",
    "        else:\n",
    "            token_lemma = token_lemma + \"_\"\n",
    "        # Add NE tags\n",
    "        if t.tokenBeginIndex in NE_BeginIndices:\n",
    "            if t.ner != \"O\":\n",
    "                # Only add tag if the word itself is an entity.\n",
    "                # (If a Pronoun refers to an entity, mention will also tag it.)\n",
    "                token_lemma = \"[NER:{}]\".format(NE_types[NE_j]) + token_lemma\n",
    "                NE_j += 1\n",
    "        sentence_parsed.append(token_lemma)\n",
    "    return \"\".join(sentence_parsed)\n",
    "\n",
    "# Preprocess text using CoreNLP parsing.\n",
    "def preprocess_text(text, client):\n",
    "    annotated_text = client.annotate(text)\n",
    "    # Process with CoreNLP and return processed text\n",
    "    return process_sentence(annotated_text)\n",
    "\n",
    "\n",
    "# Concatenates must-have phrases in the text.\n",
    "def concat_must_have_phrases(text):\n",
    "    all_seeds = []  # Load your must-have phrases here\n",
    "    pattern = \"|\".join(\n",
    "        map(re.escape, [phrase.replace(\"_\", \" \") for phrase in all_seeds])\n",
    "    )\n",
    "    text = re.sub(pattern, lambda match: match.group().replace(\" \", \"_\"), text)\n",
    "    return text\n",
    "\n",
    "# Apply trained phrase models to text.\n",
    "def apply_phrase_models(text):\n",
    "    # Apply bigram model\n",
    "    text_bigram = bigram_model[text.split()]\n",
    "    # Apply trigram model\n",
    "    text_trigram = trigram_model[text_bigram]\n",
    "    return \" \".join(text_trigram)\n",
    "\n",
    "\n",
    "# Vectorize text using the trained Word2Vec model.\n",
    "def vectorize_text(text):\n",
    "    vectorized = [w2v_model.wv[word] for word in text.split() if word in w2v_model.wv]\n",
    "    return vectorized\n",
    "\n",
    "\n",
    "#Process new text by applying all models and return its vectorized form.\n",
    "def clean_and_vectorize(doc_processed):\n",
    "    doc_cleaned = clean(doc_processed)\n",
    "    concatenated_text = concat_must_have_phrases(doc_cleaned)\n",
    "    phrase_applied_text = apply_phrase_models(concatenated_text)\n",
    "    print(phrase_applied_text)\n",
    "    vectorized_text = vectorize_text(phrase_applied_text)\n",
    "    vectorized_text = np.mean(vectorized_text, axis=0)\n",
    "    vectorized_text = vectorized_text / np.linalg.norm(vectorized_text)\n",
    "\n",
    "    # ver2.0: weighted average using tf-idf\n",
    "    # compute the tf-idf weighted average of all vectors\n",
    "    doc_weighted = []\n",
    "    for word in phrase_applied_text.split():\n",
    "        if word in w2v_model.wv:\n",
    "            word_vector = w2v_model.wv[word]\n",
    "            word_weight = np.log(1 + df_dict[word])\n",
    "            doc_weighted.append(word_vector * word_weight)\n",
    "    vectorized_text_weighted = np.mean(doc_weighted, axis=0)\n",
    "    # normalize the vector length\n",
    "    vectorized_text_weighted = vectorized_text_weighted / np.linalg.norm(\n",
    "        vectorized_text_weighted)\n",
    "    return vectorized_text, vectorized_text_weighted\n",
    "\n",
    "\n",
    "#Process new text by applying all models and return its vectorized form.\n",
    "def clean_and_vectorize(doc_processed):\n",
    "    print(doc_processed)\n",
    "    doc_cleaned = clean(doc_processed)\n",
    "    print(doc_cleaned)\n",
    "    concatenated_text = concat_must_have_phrases(doc_cleaned)\n",
    "    print(concatenated_text)\n",
    "    phrase_applied_text = apply_phrase_models(concatenated_text)\n",
    "    print(phrase_applied_text)\n",
    "    vectorized_text = vectorize_text(phrase_applied_text)\n",
    "    vectorized_text = np.mean(vectorized_text, axis=0)\n",
    "    vectorized_text = vectorized_text / np.linalg.norm(vectorized_text)\n",
    "\n",
    "    # ver2.0: weighted average using tf-idf\n",
    "    # compute the tf-idf weighted average of all vectors\n",
    "    doc_weighted = []\n",
    "    for word in phrase_applied_text.split():\n",
    "        if word in w2v_model.wv:\n",
    "            word_vector = w2v_model.wv[word]\n",
    "            word_weight = np.log(1 + df_dict[word])\n",
    "            doc_weighted.append(word_vector * word_weight)\n",
    "    vectorized_text_weighted = np.mean(doc_weighted, axis=0)\n",
    "    # normalize the vector length\n",
    "    vectorized_text_weighted = vectorized_text_weighted / np.linalg.norm(\n",
    "        vectorized_text_weighted)\n",
    "    return vectorized_text, vectorized_text_weighted\n",
    "\n",
    "doc = input('Please enter text: ')\n",
    "\n",
    "doc_processed = process_document(doc)\n",
    "vectorized_new_text = clean_and_vectorize(doc_processed)[1]\n",
    "\n",
    "marketing_aspects = [\"marketing_capabilities\", \"marketing_excellence\", \"marketing_orientation\"]\n",
    "out_put_dir = \"outputs\"\n",
    "\n",
    "def outputs(aspect):\n",
    "    dict_path = str(Path(out_put_dir, aspect, \"expanded_dict.csv\"))\n",
    "    expanded_dict = dictionary_funcs.read_dict_from_csv(dict_path)[0]\n",
    "    expanded_dict_vectors = {}\n",
    "    for dim in expanded_dict:\n",
    "        dim_vector = []\n",
    "        for word in expanded_dict[dim]:\n",
    "            if word in w2v_model.wv:\n",
    "                dim_vector.append(w2v_model.wv[word])\n",
    "        avg_dim_vector = np.mean(dim_vector, axis=0)\n",
    "        avg_dim_vector = avg_dim_vector / np.linalg.norm(avg_dim_vector)\n",
    "        expanded_dict_vectors[dim] = avg_dim_vector\n",
    "        aspect_scores = {}\n",
    "        for dim in expanded_dict_vectors:\n",
    "            aspect_scores[dim] = np.dot(vectorized_new_text, expanded_dict_vectors[dim])\n",
    "            \n",
    "    print(aspect_scores)\n",
    "\n",
    "Capabilities = outputs(\"marketing_capabilities\")\n",
    "Excellence = outputs(\"marketing_excellence\")\n",
    "Orientation = outputs(\"marketing_orientation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c848ff2-b142-4876-bab3-e8815567f6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
